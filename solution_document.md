# RTL Combinational Depth Predictor - Solution Document

## Project Information

- **Name**: Janhavi Chavada
- **Email**: janhavichavada11@gmail.com
- **GitHub**: hijanhv

## Problem Statement

Timing analysis is a crucial step in the design of any complex IP/SoC. However, timing analysis reports are generated after synthesis is complete, which is a very time-consuming process. This leads to overall delays in project execution time as timing violations can require architectural refactoring.

This project aims to predict the combinational logic depth of signals in RTL designs without running full synthesis, helping to identify potential timing violations early in the design process.

## Approach

1. **Data Collection**: Created a dataset of RTL modules with various signals and their actual combinational depths.
2. **Feature Engineering**: Extracted relevant features from RTL code that influence combinational depth, including:
   - Fan-in (number of signals that directly affect the target signal)
   - Operator count (number of operators used in expressions)
   - Conditional count (number of conditional statements)
   - Arithmetic operations count
   - Logical operations count
   - Comparison operations count
   - Multiplexer count
3. **Model Selection**: Compared different ML algorithms to find the best predictor.
4. **Training**: Trained the selected model on the dataset.
5. **Evaluation**: Evaluated the model's accuracy and performance.

## Implementation

### Project Structure

```
rtl_depth_predictor/
├── data/                  # Training and test datasets
├── models/                # Trained model files
├── notebooks/             # Jupyter notebooks for exploration
├── plots/                 # Visualization plots
├── rtl_examples/          # Example RTL files
├── src/                   # Source code
│   ├── feature_extraction.py  # RTL feature extraction
│   ├── model.py           # ML model definition
│   ├── train_model.py     # Model training script
│   └── predict_depth.py   # Prediction script
├── tests/                 # Test files
├── evaluate_model.py      # Model evaluation script
├── run_pipeline.py        # Pipeline execution script
├── requirements.txt       # Project dependencies
└── README.md              # Project documentation
```

### Feature Extraction

The feature extraction module (`src/feature_extraction.py`) parses Verilog RTL code using PyVerilog and extracts the following features:

1. **Fan-in**: Number of signals that directly affect the target signal.
2. **Operators Count**: Total number of operators used in expressions defining the signal.
3. **Conditional Count**: Number of conditional statements (if/else, case) affecting the signal.
4. **Arithmetic Operations**: Count of arithmetic operators (+, -, \*, /, etc.).
5. **Logical Operations**: Count of logical operators (&, |, ^, ~, etc.).
6. **Comparison Operations**: Count of comparison operators (==, !=, <, >, etc.).
7. **Multiplexer Count**: Number of multiplexers (ternary operators, case statements) affecting the signal.

### Model Selection

We compared several machine learning models:

- Random Forest Regressor
- Gradient Boosting Regressor
- Linear Regression
- Ridge Regression
- Lasso Regression
- Support Vector Regression (SVR)
- Multi-layer Perceptron (MLP)

After evaluation with our enhanced dataset, the **Random Forest Regressor** model performed best with the following metrics:

- MSE: 0.7233
- MAE: 0.4910
- R²: 0.8375
- Within ±1 depth level: 86.67%

## Results

### Model Performance

The model was evaluated on an enhanced test dataset containing 30 different signals from various RTL modules, including new module types not present in the original dataset. The results show that:

1. The model achieves an R² score of 0.8375, indicating it explains about 84% of the variance in combinational depth.
2. The mean absolute error (MAE) is 0.4910, meaning predictions are off by less than 0.5 depth level on average.
3. 86.67% of predictions are within ±1 of the actual depth, which is excellent for early timing violation detection.

### Dataset Enhancement

We significantly improved the model's performance by enhancing the dataset:

1. **Enhanced Training Data**:

   - Added 60 new examples covering 5 additional module types (FPU, DSP, Video Processor, Crypto Engine, PCIe Controller, and Cache Controller)
   - Included more complex circuits with higher combinational depths (up to 12)
   - Added more diverse signal types with various combinations of operators

2. **Enhanced Test Data**:
   - Added 12 new test examples from the new module types
   - Included signals with higher combinational depths to test the model's ability to predict more complex circuits

### Prediction Examples

Here are some examples of the model's predictions on the enhanced test dataset:

| Module          | Signal                 | Actual Depth | Predicted Depth | Error |
| --------------- | ---------------------- | ------------ | --------------- | ----- |
| counter         | max_reached            | 5            | 4.97            | 0.03  |
| alu             | overflow_flag          | 7            | 6.54            | 0.46  |
| fpu             | transcendental_result  | 9            | 8.73            | 0.27  |
| dsp             | adaptive_filter_output | 9            | 8.82            | 0.18  |
| crypto_engine   | ecc_signature          | 11           | 10.45           | 0.55  |
| pcie_controller | flow_control_credits   | 6            | 5.78            | 0.22  |

### Challenges and Limitations

1. **Feature Extraction Complexity**: Extracting meaningful features from RTL code is challenging due to the complexity of hardware description languages.
2. **Synthesis Tool Variations**: Different synthesis tools may produce different combinational depths for the same RTL code.
3. **Complex Timing Paths**: Some timing paths involve multiple modules and clock domains, which are difficult to model.
4. **Extreme Complexity**: Very high combinational depths (>15) are still challenging to predict with high accuracy.

## Future Improvements

1. **Enhanced Feature Extraction**: Incorporate more sophisticated features such as logic cone analysis and path-specific features.
2. **Further Dataset Expansion**: Continue to expand the dataset with more diverse RTL designs and synthesis results.
3. **Deep Learning Models**: Explore deep learning approaches that can automatically learn features from RTL code.
4. **Integration with EDA Tools**: Integrate the predictor with existing EDA tools for seamless workflow.
5. **Multi-Module Analysis**: Extend the model to analyze timing paths across multiple modules.

## Conclusion

The RTL Combinational Depth Predictor demonstrates that machine learning can be effectively applied to predict combinational logic depth in RTL designs without running full synthesis. With an accuracy of 86.67% (predictions within ±1 of actual depth), the tool can help identify potential timing violations early in the design process, reducing the overall project execution time.

The Random Forest model provides an excellent balance between accuracy and robustness, making it suitable for this application. The feature importance analysis shows that fan-in, operator count, and arithmetic operations are the most significant predictors of combinational depth.

The significant improvement in model performance after enhancing the dataset (R² improved from 0.5119 to 0.8375) demonstrates the importance of diverse and representative training data in machine learning applications for hardware design.

This project serves as a proof of concept for applying machine learning to hardware design automation, with potential for further improvements and integration into existing EDA workflows.
